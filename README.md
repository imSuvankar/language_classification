# A COMPARATIVE STUDY ON ENCODING SCHEMES TOWARDS LANGUAGE DETECTION

> #### <i>Suvankar Das (suvankar_das@outlook.com)<br>Guide: Dr. Uddalak Mitra<br>Siliguri Institute of Technology, Siliguri, WB, IND<br><br>April, 2023</i>

<br>

This paper presents an investigation of language identification using various encoding methods and machine learning algorithms. Specifically, we compared five encoding methods: 
```
1. Bag of Words 
2. TF-IDF
3. Word2Vec
4. FastText
5. GloVe 
```
and 15 machine learning models:

```
1. Multinomial Naive Bayes
2. Support Vector Machine
3. Random Forest
4. Gradient Boosting Classifier
5. Decision Tree Classifier
6. K-Nearest Neighbors Classifier
7. AdaBoost Classifier
8. Logistic Regression
9. Extra Trees Classifier
10. Gaussian Process Regressor
11. Ridge Regressor
12. ElasticNet Regressor
13. Multilayer Perceptron
14. Lasso Regression
15. XG Boost
```

We evaluated the performance of each combination on four different datasets with different language distributions. Our study found that the FastText encoding method combined with the Extra Trees Classifier model outperformed the Bag of Words encoding method combined with the Multinomial Naive Bayes model despite being the most used combination in language detection tasks. Furthermore, we discovered that the choice of encoding method and machine learning model greatly impacts the accuracy of language identification, with some combinations performing significantly better than others. 

This research contributes to the field of language identification by giving insights into the performance of various encoding methods and ML models. The findings of this study can help the development of language identification systems, which are essential for many applications such as online content filtering, targeted marketing, and natural language processing.
